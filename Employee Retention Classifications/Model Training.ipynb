{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:42px; text-align:center; margin-bottom:30px;\"><span style=\"color:SteelBlue\"></span> Model Training</h1>\n",
    "<hr>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import libraries, recruit models, and load the ABT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "# Scikit-Learn for Modeling\n",
    "import sklearn\n",
    "# Pickle for saving model files\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# Classification metrics \n",
    "from sklearn.metrics import auc, confusion_matrix, roc_curve, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analytical base table from Module 2\n",
    "df = pd.read_csv('analytical-base-table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"split\"></span>\n",
    "# Split dataset\n",
    "\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.status\n",
    "\n",
    "# Create separate object for input features\n",
    "X = df.drop('status', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11254, 25)\n",
      "(2814, 25)\n",
      "(11254,)\n",
      "(2814,)\n"
     ]
    }
   ],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234, stratify = df.status)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "for dataframe in (X_train, X_test, y_train, y_test):\n",
    "    print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"pipelines\"></span>\n",
    "# Build model pipelines\n",
    "\n",
    "Next, let's set up preprocessing pipelines for each of our algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline dictionary\n",
    "pipelines = {\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l1', random_state = 123)),\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l2', random_state = 123)),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state = 123)),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state = 123)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"hyperparameters\"></span>\n",
    "# Declare hyperparameters to tune\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               presort='auto', random_state=123, subsample=1.0, verbose=0,\n",
       "               warm_start=False),\n",
       " 'gradientboostingclassifier__criterion': 'friedman_mse',\n",
       " 'gradientboostingclassifier__init': None,\n",
       " 'gradientboostingclassifier__learning_rate': 0.1,\n",
       " 'gradientboostingclassifier__loss': 'deviance',\n",
       " 'gradientboostingclassifier__max_depth': 3,\n",
       " 'gradientboostingclassifier__max_features': None,\n",
       " 'gradientboostingclassifier__max_leaf_nodes': None,\n",
       " 'gradientboostingclassifier__min_impurity_decrease': 0.0,\n",
       " 'gradientboostingclassifier__min_impurity_split': None,\n",
       " 'gradientboostingclassifier__min_samples_leaf': 1,\n",
       " 'gradientboostingclassifier__min_samples_split': 2,\n",
       " 'gradientboostingclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'gradientboostingclassifier__n_estimators': 100,\n",
       " 'gradientboostingclassifier__presort': 'auto',\n",
       " 'gradientboostingclassifier__random_state': 123,\n",
       " 'gradientboostingclassifier__subsample': 1.0,\n",
       " 'gradientboostingclassifier__verbose': 0,\n",
       " 'gradientboostingclassifier__warm_start': False,\n",
       " 'memory': None,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('gradientboostingclassifier',\n",
       "   GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                 learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                 max_features=None, max_leaf_nodes=None,\n",
       "                 min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                 min_samples_leaf=1, min_samples_split=2,\n",
       "                 min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                 presort='auto', random_state=123, subsample=1.0, verbose=0,\n",
       "                 warm_start=False))]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tuneable hyperparameters of our Logistic pipeline\n",
    "pipelines['gb'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare the **hyperparameter grids** to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameters\n",
    "l1_hyperparameters = {\n",
    "    'logisticregression__C' :  np.linspace(1e-3, 1e3, 10)\n",
    "}\n",
    "\n",
    "l2_hyperparameters = {\n",
    "    'logisticregression__C' :  np.linspace(1e-3, 1e3, 10) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare the hyperparameter grid for the random forest.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = {\n",
    "                'randomforestclassifier__n_estimators': [100, 200],\n",
    "                'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33]\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare the hyperparameter grid for the boosted tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Tree hyperparameters\n",
    "gb_hyperparameters = {\n",
    "  'gradientboostingclassifier__n_estimators': [100, 200],\n",
    "   'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'gradientboostingclassifier__max_depth' : [1,3,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    'l1': l1_hyperparameters,\n",
    "    'l2': l2_hyperparameters,\n",
    "    'rf': rf_hyperparameters,\n",
    "    'gb': gb_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 was found in hyperparameters, and it is a grid.\n",
      "l2 was found in hyperparameters, and it is a grid.\n",
      "rf was found in hyperparameters, and it is a grid.\n",
      "gb was found in hyperparameters, and it is a grid.\n"
     ]
    }
   ],
   "source": [
    "for key in ['l1', 'l2', 'rf', 'gb']:\n",
    "    if key in hyperparameters:\n",
    "        if type(hyperparameters[key]) is dict:\n",
    "            print( key, 'was found in hyperparameters, and it is a grid.' )\n",
    "        else:\n",
    "            print( key, 'was found in hyperparameters, but it is not a grid.' )\n",
    "    else:\n",
    "        print( key, 'was not found in hyperparameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"fit-tune\"></span>\n",
    "# Fit and tune models with cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 has been fitted\n",
      "l2 has been fitted\n",
      "rf has been fitted\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "        model = GridSearchCV(pipeline, hyperparameters[name], cv = 10, n_jobs = -1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "        fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "        print(\"{} has been fitted\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the models are of the correct type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that we have 5 cross-validation objects\n",
    "for key, value in fitted_models.items():\n",
    "    print( key, type(value) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make sure that the models have been fitted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "for name, model in fitted_models.items():\n",
    "    try:\n",
    "        pred = model.predict(X_test)\n",
    "        print(name, 'has been fitted.')\n",
    "    except NotFittedError as e:\n",
    "        print(repr(e))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"evaluate\"></span>\n",
    "# Evaluate metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best_score_ for each fitted model\n",
    "for name, model in fitted_models.items():\n",
    "    print(name, model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using L1-regularized logistic regression \n",
    "l1_pred = fitted_models['l1'].predict(X_test)\n",
    "\n",
    "# Display first 5 predictions\n",
    "l1_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display confusion matrix for y_test and pred\n",
    "print(confusion_matrix(y_test, l1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using L1-regularized logistic regression\n",
    "pred = fitted_models['l1'].predict_proba(X_test)\n",
    "\n",
    "# Get just the prediction for the positive class (1)\n",
    "pred = [p[1] for p in pred]\n",
    "\n",
    "# Display first 5 predictions\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve from y_test and pred\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store fpr, tpr, thresholds in DataFrame and display last 10\n",
    "roc_df = pd.DataFrame({'False Positive Rate': fpr, 'True Positive Rate' : tpr, 'Thresholds': thresholds})\n",
    "roc_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We can plot the entire curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "sns.set_style('darkgrid')\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.title('Receiver Operating Characteristic for logistic regression (L1)', fontsize = 18)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='l1')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "# Diagonal 45 degree line\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "\n",
    "# Axes limits and labels\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate', fontsize = 16)\n",
    "plt.xlabel('False Positive Rate', fontsize = 16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve for Logistic Regression (l1)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "\n",
    "# Calculate AUROC for Logistic Regression (l1)\n",
    "print(auc(fpr, tpr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty Dictionaries for fpr, tpr, auc, and kappa\n",
    "fpr_dic = {}\n",
    "tpr_dic = {}\n",
    "auc_dic = {}\n",
    "kappa_dic = {}\n",
    "\n",
    "# For loop to collect fpr, tpr and auc for each model and print out the auc\n",
    "for name, model in fitted_models.items():\n",
    "    pred_probs = model.predict_proba(X_test)\n",
    "    pred_probs = [p[1] for p in pred_probs]\n",
    "    pred_labels = model.predict(X_test)\n",
    "    fpr_dic[name], tpr_dic[name], thresholds = roc_curve(y_test, pred_probs)\n",
    "    auc_dic[name] = auc(fpr_dic[name], tpr_dic[name])\n",
    "    kappa_dic[name] = cohen_kappa_score(y_test, pred_labels)\n",
    "    print(name, auc_dic[name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr_dic['l1'], tpr_dic['l1'],\n",
    "         label='Logistic Regression (L1) (auc = %0.3f)' % auc_dic['l1'], color = 'darkorange')\n",
    "plt.plot(fpr_dic['l2'], tpr_dic['l2'],\n",
    "         label='Logistic Regression (L2) (auc = %0.3f)' % auc_dic['l2'], color = 'darkgreen')\n",
    "plt.plot(fpr_dic['rf'], tpr_dic['rf'],\n",
    "         label='Random Forest (auc = %0.3f)' % auc_dic['rf'], color = 'darkred')\n",
    "plt.plot(fpr_dic['gb'], tpr_dic['gb'],\n",
    "         label='Gradient Booster (auc = %0.3f)' % auc_dic['gb'], color = 'blue')\n",
    "\n",
    "# Diagonal 45 degree line\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "\n",
    "# Axes limits and labels\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate', fontsize = 16)\n",
    "plt.xlabel('False Positive Rate', fontsize = 16)\n",
    "plt.title('ROC Curve', fontsize = 18)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kappa Scores and AUC Dataframe\n",
    "kappa_list = list(kappa_dic.values())\n",
    "auc_list = list(auc_dic.values())\n",
    "\n",
    "# Formatting the lists\n",
    "kappa_list = [ '%.3f' % elem for elem in kappa_list]\n",
    "auc_list = [ '%.3f' % elem for elem in auc_list]\n",
    "\n",
    "# Calculate kappa scores for each model\n",
    "print(pd.DataFrame({'Kappa' : kappa_list,\n",
    "                   'AUC' : auc_list,\n",
    "                   'Model': ['Logistic Regression (L1)',\n",
    "                              'Logistic Regression (L2)',\n",
    "                              'Random Forest',\n",
    "                              'Gradient Boosting']}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best model seems to be Random Forest with an AUC of 0.991 and Kappa score of 0.943.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our winning model's parameters\n",
    "fitted_models['rf'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance using Random Forest\n",
    "rf_model = RandomForestClassifier(bootstrap=True, criterion='gini', max_depth=None,\n",
    "           max_features=0.33, max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
    "           oob_score=False, random_state=123, verbose=0, warm_start=False)\n",
    "\n",
    "## Fit the model on training data.\n",
    "rf_model.fit(X_train, y_train) \n",
    "## And score it on testing data.\n",
    "rf_model.score(X_test, y_test)\n",
    "# Store it into a dataframe\n",
    "imp_features = pd.DataFrame({'importance' : rf_model.feature_importances_ ,\n",
    "                         'feature' : X_train.columns.values}).sort_values('importance', ascending = False)\n",
    "# Display top 5 features                                                                       \n",
    "imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [imp_features.feature[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\", fontsize = 18)\n",
    "\n",
    "# Add bars\n",
    "plt.barh(range(X_train.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as y-axis labels\n",
    "plt.yticks(range(X_train.shape[1]), names, fontsize = 12)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save winning model as final_model.pkl\n",
    "with open('final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(fitted_models['rf'].best_estimator_, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
